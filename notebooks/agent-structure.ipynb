{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nj/mkjg2k_d4hv0th92zft43hmw0000gn/T/ipykernel_18583/2449679658.py:2: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from utils.langchain_agent_service import LangchainJSONEngine, LangchainSimpleEngine\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.gemini_service import GeminiModel, GeminiJsonEngine, GeminiSimpleChatEngine\n",
    "from utils.langchain_agent_service import LangchainJSONEngine, LangchainSimpleEngine\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(path: str) -> Dict[str, Any]:\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    with open(path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def save_json_data(path: str, data: Dict[str, Any]):\n",
    "    # Create the directory if it doesn't exist\n",
    "    directory = os.path.dirname(path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    with open(path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain LLM Engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini Engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/debasmitroy/Desktop/programming/gemini-agent-assist/key.json\"\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"hackathon0-project\"\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pydantic Basemodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment(BaseModel):\n",
    "    \"\"\"\n",
    "    This tool is used to analyze the sentiment of a given text. The sentiment is analyzed based on the emotions of the text.\n",
    "    \"\"\"\n",
    "    happy: bool = Field(title=\"Happy\",description=\"The User is happy.\")\n",
    "    sad: bool = Field(title=\"Sad\",description=\"The User is sad.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_engine = LangchainJSONEngine(\n",
    "#     sampleBaseModel=Sentiment,\n",
    "#     systemPromptText=\"You are an AI assistant. You are helping a user with a task. The user is asking you questions and you are answering them.\",\n",
    "#     temperature=0.0\n",
    "# )\n",
    "\n",
    "# result = sentiment_engine.run(\"I am happy\")\n",
    "# print(dict(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemini_sentiment_engine = GeminiJsonEngine(\n",
    "#                                     model_name=\"gemini-2.0-flash-001\",\n",
    "#                                     basemodel=Sentiment,\n",
    "#                                     temperature=0.5,\n",
    "#                                     max_output_tokens=256,\n",
    "#                                     systemInstructions=None,\n",
    "#                                     max_retries=5,\n",
    "#                                     wait_time=30\n",
    "#                                     )\n",
    "\n",
    "# # Not that good\n",
    "# gemini_sentiment_engine(\n",
    "#     [\n",
    "#         \"You are an AI assistant. Your task is to analyze the sentiment of the user's text.\",\n",
    "#         \"Now analyze the sentiment of the user's text. Generate sentiment scores for anger, joy, and fear form the user's text. Use `Sentiment` as the base model. Stricly follow the arguments and return the result in the form of a JSON object.\",\n",
    "#         \"User: I am happy. I am very happy today.\"\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend Anl Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytrends.request import TrendReq\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "from collections import defaultdict, Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchTokenModel(BaseModel):\n",
    "    \"\"\"\n",
    "    This model extracts search tokens based on the brand's market scope, TAM, etc.\n",
    "    \"\"\"\n",
    "    search_tokens: list[str] = Field(\n",
    "        title=\"Search Tokens\",\n",
    "        description=\"List of search tokens in the form of common nouns or search queries relevant to the brand. These tokens should be meanningful phrase not a single word.\"\n",
    "    )\n",
    "\n",
    "class DummyTrendModel(BaseModel):\n",
    "    country: str = Field(title=\"Country\", description=\"Country code (e.g., US, IN, etc.)\")\n",
    "    keyword: str = Field(title=\"Keyword\", description=\"The keyword for which trends are simulated.\")\n",
    "    trendscore: int = Field(title=\"Trend Score\", description=\"Peak interest score (0-100).\")\n",
    "    top_months: list[str] = Field(title=\"Top Months\", description=\"Top 3 months where the trend peaked (YYYY-MM format).\")\n",
    "    top_states: list[str] = Field(title=\"Top States\", description=\"Top 3 states/regions within the country.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrendAnalyzer:\n",
    "    def __init__(self):\n",
    "        # Engine to generate search tokens\n",
    "        self.token_engine = LangchainJSONEngine(\n",
    "            sampleBaseModel=SearchTokenModel,\n",
    "            systemPromptText=\"\"\"\n",
    "            You are a marketing strategist helping a brand identify the most effective search terms for trend analysis. \n",
    "            Based on the brand description, TAM, and market scope, generate a list of highly relevant search tokens (common noun queries). \n",
    "            The tokens should be simple, commonly searched phrases like \"electric scooter in India\", \"eco-friendly scooter\", \"best commuter scooter\", etc. \n",
    "            Focus on intent-based keywords that would likely appear in Google Trends.\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        # Dummy trend data generator\n",
    "        self.dummy_engine = LangchainJSONEngine(\n",
    "            sampleBaseModel=DummyTrendModel,\n",
    "            systemPromptText=\"\"\"\n",
    "            You are simulating trend data for a market research report. \n",
    "            Given a country and a search token, provide:\n",
    "            - a realistic peak trendscore (between 60 to 100),\n",
    "            - top 3 months in the format \"YYYY-MM\" in 2024.\n",
    "            - top 3 states/regions where this search term is popular within the country.\n",
    "            The data should appear realistic and well-distributed.\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        self.countries = ['US', 'IN']\n",
    "\n",
    "\n",
    "    def _get_countrywise_top_states_per_month(self,trend_data):\n",
    "        country_month_state_counter = defaultdict(lambda: defaultdict(Counter))\n",
    "\n",
    "        # Step 1: Aggregate data\n",
    "        for entry in trend_data:\n",
    "            for country, info in entry.items():\n",
    "                months = info['top-months']\n",
    "                states = info['top-states']\n",
    "                for month in months:\n",
    "                    country_month_state_counter[country][month].update(states)\n",
    "\n",
    "        # Step 2: Pick top state(s) for each country-month\n",
    "        result = defaultdict(dict)\n",
    "        for country, month_data in country_month_state_counter.items():\n",
    "            for month, state_counter in month_data.items():\n",
    "                max_count = max(state_counter.values())\n",
    "                top_states = [state for state, count in state_counter.items() if count == max_count]\n",
    "                month_name = datetime.strptime(month, \"%Y-%m\").strftime(\"%B\")\n",
    "                result[country][month_name] = top_states\n",
    "\n",
    "        return dict(result)\n",
    "\n",
    "\n",
    "    def run(self, description: str):\n",
    "        # Step 1: Extract search tokens from brand description\n",
    "        tokens_result = self.token_engine.run(description)\n",
    "        search_tokens = tokens_result.search_tokens\n",
    "\n",
    "        # Step 2: Dummy trend simulation instead of actual API calls\n",
    "        output = []\n",
    "\n",
    "        for country in self.countries:\n",
    "            for keyword in search_tokens:\n",
    "                # print(f\"Country: {country}, Keyword: {keyword}\")\n",
    "                dummy_data = self.dummy_engine.run(f\"Country: {country}, Keyword: {keyword}\")\n",
    "                output.append({\n",
    "                    dummy_data.country: {\n",
    "                        \"keyword\": dummy_data.keyword,\n",
    "                        \"trendscore\": dummy_data.trendscore,\n",
    "                        \"top-months\": dummy_data.top_months,\n",
    "                        \"top-states\": dummy_data.top_states\n",
    "                    }\n",
    "                })\n",
    "        \n",
    "        # Step 3: Post-process the data\n",
    "        result = self._get_countrywise_top_states_per_month(output)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debasmitroy/Desktop/programming/elastic-openserve/backend/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:1413: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n",
      "/Users/debasmitroy/Desktop/programming/elastic-openserve/backend/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:1413: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trend_analyzer = TrendAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"\"\"\n",
    "    Our brand is ElecX, revolutionizing electric scooters. \n",
    "    We are passionate about creating fast, eco-friendly, and commuter-friendly scooters. \n",
    "    The new ElecX Pro targets urban mobility solutions worldwide.\n",
    "    \"\"\"\n",
    "trend_result = trend_analyzer.run(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'US': {'June': ['California', 'Texas', 'New York'],\n",
       "  'September': ['California', 'Texas', 'New York'],\n",
       "  'November': ['California', 'Texas', 'New York'],\n",
       "  'February': ['California', 'New York', 'Texas']},\n",
       " 'IN': {'March': ['Maharashtra', 'Karnataka'],\n",
       "  'June': ['Maharashtra', 'Karnataka'],\n",
       "  'September': ['Maharashtra', 'Karnataka'],\n",
       "  'November': ['Maharashtra', 'Karnataka', 'Tamil Nadu'],\n",
       "  'February': ['Maharashtra', 'Karnataka', 'Tamil Nadu']}}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trend_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sitelog Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sql_engine_service import get_sitelog_inmemory_db,SITELOG_INMEM_DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "SITELOG_INMEM_DB = get_sitelog_inmemory_db(load_json_data(\"data/sitelog.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiteLogQuery(BaseModel):\n",
    "    text_query: str = Field(title=\"Text Query\", description=\"The text query to search in the site log.\")\n",
    "    sql_query: str = Field(title=\"SQL Query\", description=\"Corresponding SQL query to search in the site log.\")\n",
    "\n",
    "class SiteLogQueries(BaseModel):\n",
    "    queries: List[SiteLogQuery] = Field(title=\"Site Log Queries\", description=\"List of text queries and their corresponding SQL queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiteLogAgent:\n",
    "    def __init__(self, target_product_id: str):\n",
    "        global SITELOG_INMEM_DB\n",
    "        SITELOG_INMEM_DB_COLS, SITELOG_INMEM_DB_HEAD = SITELOG_INMEM_DB.query_data(\"SELECT * FROM sitelog LIMIT 5\")\n",
    "        self.PD_SITELOG_INMEM_DB_HEAD = pd.DataFrame(SITELOG_INMEM_DB_HEAD, columns=SITELOG_INMEM_DB_COLS)\n",
    "\n",
    "        self.engine = GeminiJsonEngine(\n",
    "                                    model_name=\"gemini-2.0-flash-001\",\n",
    "                                    basemodel=SiteLogQueries,\n",
    "                                    temperature=0.5,\n",
    "                                    max_output_tokens=1024,\n",
    "                                    systemInstructions=None,\n",
    "                                    max_retries=5,\n",
    "                                    wait_time=30\n",
    "                                    )\n",
    "        \n",
    "    def run(self,target_product_id):\n",
    "\n",
    "        # Validate the product ID\n",
    "        PRODUCT_ID_COUNT = SITELOG_INMEM_DB.query_data(f\"SELECT COUNT(*) FROM sitelog WHERE product_id = '{target_product_id}'\")\n",
    "        if PRODUCT_ID_COUNT[1][0][0] == 0:\n",
    "            raise ValueError(f\"Product ID '{target_product_id}' not found in the site log.\")\n",
    "\n",
    "        queries_result = self.engine(\n",
    "            [\n",
    "                \"You are a SQL expert. Your task is to write a SQL script to query data from the given table. Note: you are generating a SQL script for SQLLite's python library. You must be careful while writing complex queries as it is very sensitive.\",\n",
    "                f\"Here are the first few rows of the table sitelog: {self.PD_SITELOG_INMEM_DB_HEAD}.\",\n",
    "                f\"Now generate a list of text queries and their corresponding SQL queries to search in the site log to fetch some useful information and groupings.\",\n",
    "                f\"Example: From which regions are most of the users purchasing the product with product_id = {target_product_id}? Give me the percentage of users from each region.\",\n",
    "                f\"SQL Query: SELECT region, COUNT(*) * 100.0 / (SELECT COUNT(*) as PERCENT FROM sitelog WHERE product_id = '{target_product_id}') as percentage FROM sitelog WHERE product_id = '{target_product_id}' GROUP BY region ORDER BY percentage DESC;\",\n",
    "                f\"You are allowed to write multiple queries. Make sure to provide the text query and the corresponding SQL query in the response.\",\n",
    "                f\"Always extract percentage not count. Also, make sure to order the results in both TOP and BOTTOM order with limit 3.\",\n",
    "                f\"Use different groupings based on Age, Demography, Gender, Month etc.\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        final_result = []\n",
    "        succes = 0\n",
    "        for query in queries_result[0]['queries']:\n",
    "            try:\n",
    "                result = SITELOG_INMEM_DB.query_data(query['sql_query'])\n",
    "                succes += 1\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            final_result.append({\n",
    "                \"text_query\": query['text_query'],\n",
    "                \"sql_query\": query['sql_query'],\n",
    "                \"result\": result\n",
    "            })\n",
    "        \n",
    "        print(f\"Successful queries: {succes} out of {len(queries_result[0]['queries'])}\")\n",
    "        return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m2025-03-16 23:41:40,131 - DEBUG ==> Initialized GeminiModel with model gemini-2.0-flash-001 , project hackathon0-project, location us-central1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SiteLogAgent\n",
    "site_log_agent=SiteLogAgent(target_product_id=\"P001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful queries: 8 out of 8\n"
     ]
    }
   ],
   "source": [
    "site_log_reuslt = site_log_agent.run(target_product_id=\"P001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text_query': 'What are the top 3 and bottom 3 regions by percentage of total transactions?',\n",
       "  'sql_query': 'SELECT region, COUNT(*) * 100.0 / (SELECT COUNT(*) FROM sitelog) AS percentage FROM sitelog GROUP BY region ORDER BY percentage DESC LIMIT 3;',\n",
       "  'result': (['region', 'percentage'],\n",
       "   [['Maharashtra', 18.181818181818183],\n",
       "    ['Texas', 15.151515151515152],\n",
       "    ['Karnataka', 15.151515151515152]])},\n",
       " {'text_query': 'What are the bottom 3 regions by percentage of total transactions?',\n",
       "  'sql_query': 'SELECT region, COUNT(*) * 100.0 / (SELECT COUNT(*) FROM sitelog) AS percentage FROM sitelog GROUP BY region ORDER BY percentage ASC LIMIT 3;',\n",
       "  'result': (['region', 'percentage'],\n",
       "   [['', 3.0303030303030303],\n",
       "    ['Berlin', 3.0303030303030303],\n",
       "    ['Sao Paulo', 3.0303030303030303]])},\n",
       " {'text_query': 'What are the top 3 age groups by percentage of total transactions?',\n",
       "  'sql_query': 'SELECT age, COUNT(*) * 100.0 / (SELECT COUNT(*) FROM sitelog) AS percentage FROM sitelog GROUP BY age ORDER BY percentage DESC LIMIT 3;',\n",
       "  'result': (['age', 'percentage'],\n",
       "   [['22', 9.090909090909092],\n",
       "    ['19', 9.090909090909092],\n",
       "    ['30', 6.0606060606060606]])},\n",
       " {'text_query': 'What are the bottom 3 age groups by percentage of total transactions?',\n",
       "  'sql_query': 'SELECT age, COUNT(*) * 100.0 / (SELECT COUNT(*) FROM sitelog) AS percentage FROM sitelog GROUP BY age ORDER BY percentage ASC LIMIT 3;',\n",
       "  'result': (['age', 'percentage'],\n",
       "   [['18', 3.0303030303030303],\n",
       "    ['20', 3.0303030303030303],\n",
       "    ['25', 3.0303030303030303]])},\n",
       " {'text_query': 'What are the top 3 genders by percentage of total transactions?',\n",
       "  'sql_query': 'SELECT gender, COUNT(*) * 100.0 / (SELECT COUNT(*) FROM sitelog) AS percentage FROM sitelog GROUP BY gender ORDER BY percentage DESC LIMIT 3;',\n",
       "  'result': (['gender', 'percentage'],\n",
       "   [['Male', 57.57575757575758],\n",
       "    ['Female', 39.39393939393939],\n",
       "    ['', 3.0303030303030303]])},\n",
       " {'text_query': 'What are the bottom 3 genders by percentage of total transactions?',\n",
       "  'sql_query': 'SELECT gender, COUNT(*) * 100.0 / (SELECT COUNT(*) FROM sitelog) AS percentage FROM sitelog GROUP BY gender ORDER BY percentage ASC LIMIT 3;',\n",
       "  'result': (['gender', 'percentage'],\n",
       "   [['', 3.0303030303030303],\n",
       "    ['Female', 39.39393939393939],\n",
       "    ['Male', 57.57575757575758]])},\n",
       " {'text_query': 'What are the top 3 months by percentage of total transactions?',\n",
       "  'sql_query': \"SELECT strftime('%Y-%m', date) AS month, COUNT(*) * 100.0 / (SELECT COUNT(*) FROM sitelog) AS percentage FROM sitelog GROUP BY month ORDER BY percentage DESC LIMIT 3;\",\n",
       "  'result': (['month', 'percentage'],\n",
       "   [['2025-02', 54.54545454545455],\n",
       "    ['2025-03', 27.272727272727273],\n",
       "    ['2025-01', 12.121212121212121]])},\n",
       " {'text_query': 'What are the bottom 3 months by percentage of total transactions?',\n",
       "  'sql_query': \"SELECT strftime('%Y-%m', date) AS month, COUNT(*) * 100.0 / (SELECT COUNT(*) FROM sitelog) AS percentage FROM sitelog GROUP BY month ORDER BY percentage ASC LIMIT 3;\",\n",
       "  'result': (['month', 'percentage'],\n",
       "   [['2024-12', 6.0606060606060606],\n",
       "    ['2025-01', 12.121212121212121],\n",
       "    ['2025-03', 27.272727272727273]])}]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_log_reuslt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SiteLogRefinerAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionDemography(BaseModel):\n",
    "    state: str = Field(title=\"State\", description=\"The state or region name.\")\n",
    "    country: str = Field(title=\"Country\", description=\"The country name.\")\n",
    "\n",
    "class SiteLogRefinedResult(BaseModel):\n",
    "    \"\"\"\n",
    "    This model represents the refined results of the site\n",
    "    \"\"\"\n",
    "    top2_months: List[str] = Field(title=\"Top 2 Months\", description=\"Top 2 months with the highest number of user interactions. Not year specific. Connvert month number to month name.\")\n",
    "    top_age_group: str = Field(title=\"Top Age Group\", description=\"Top age group with the highest number of user interactions. Example: 18-24, 25-34, etc.\")\n",
    "    top2_regions: List[RegionDemography] = Field(title=\"Top 2 Regions\", description=\"Top 2 regions with the highest number of user interactions.\")\n",
    "\n",
    "    bottom2_months: List[str] = Field(title=\"Bottom 2 Months\", description=\"Bottom 2 months with the lowest number of user interactions. Not year specific. Connvert month number to month name.\")\n",
    "    bottom_age_group: str = Field(title=\"Bottom Age Group\", description=\"Bottom age group with the lowest number of user interactions. Example: 18-24, 25-34, etc.\")\n",
    "    bottom2_regions: List[RegionDemography] = Field(title=\"Bottom 2 Regions\", description=\"Bottom 2 regions with the lowest number of user interactions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiteLogRefinerAgent:\n",
    "    def __init__(self):\n",
    "        self.engine = LangchainJSONEngine(\n",
    "            sampleBaseModel=SiteLogRefinedResult,\n",
    "            systemPromptText=\"\"\"\n",
    "            You are a data analyst. Your task is to analyze the site log data to extract meaningful insights.\n",
    "            \"\"\",\n",
    "            temperature=0.2\n",
    "        )\n",
    "\n",
    "    def run(self, site_log_result):\n",
    "        parsed_result = \"\\n\".join([f\"{res['text_query']}\\nResult: {res['result']}\" for res in site_log_result])\n",
    "        result = self.engine.run(f\"\"\"These are the results of the SQL queries on the site log data: {parsed_result}\n",
    "        Analyze the data and provide the following insights:\n",
    "        - Top/Bottom 2 months with the highest number of user interactions.\n",
    "        - Top/Bottom 2 regions with the highest number of user interactions.\n",
    "        - Top/Bottom age group with the highest number of user interactions.\n",
    "        - Top/Bottom region with the highest number of user interactions.\n",
    "        \"\"\")\n",
    "\n",
    "        result_dict = dict(result)\n",
    "        # Convert List(top2_region) to List[Dict]\n",
    "        top2_regions = [dict(region) for region in result_dict['top2_regions']]\n",
    "        result_dict['top2_regions'] = top2_regions\n",
    "\n",
    "        # Convert List(bottom2_region) to List[Dict]\n",
    "        bottom2_regions = [dict(region) for region in result_dict['bottom2_regions']]\n",
    "        result_dict['bottom2_regions'] = bottom2_regions\n",
    "\n",
    "        # Top Vs Bottom Results \n",
    "        top_bottom_results = {\n",
    "            \"top\": {\n",
    "                \"months\": result_dict['top2_months'],\n",
    "                \"age_group\": result_dict['top_age_group'],\n",
    "                \"regions\": result_dict['top2_regions']\n",
    "            },\n",
    "            \"bottom\": {\n",
    "                \"months\": result_dict['bottom2_months'],\n",
    "                \"age_group\": result_dict['bottom_age_group'],\n",
    "                \"regions\": result_dict['bottom2_regions']\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return top_bottom_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debasmitroy/Desktop/programming/elastic-openserve/backend/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:1413: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "site_log_refiner_agent = SiteLogRefinerAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_log_refiner_result = site_log_refiner_agent.run(site_log_reuslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'top': {'months': ['February', 'March'],\n",
       "  'age_group': '22',\n",
       "  'regions': [{'state': 'Maharashtra', 'country': ''},\n",
       "   {'state': 'Texas', 'country': ''}]},\n",
       " 'bottom': {'months': ['December', 'January'],\n",
       "  'age_group': '18',\n",
       "  'regions': [{'state': '', 'country': ''},\n",
       "   {'state': 'Berlin', 'country': ''}]}}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_log_refiner_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now We Have Two Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'US': {'June': ['California', 'Texas', 'New York'],\n",
       "  'September': ['California', 'Texas', 'New York'],\n",
       "  'November': ['California', 'Texas', 'New York'],\n",
       "  'February': ['California', 'New York', 'Texas']},\n",
       " 'IN': {'March': ['Maharashtra', 'Karnataka'],\n",
       "  'June': ['Maharashtra', 'Karnataka'],\n",
       "  'September': ['Maharashtra', 'Karnataka'],\n",
       "  'November': ['Maharashtra', 'Karnataka', 'Tamil Nadu'],\n",
       "  'February': ['Maharashtra', 'Karnataka', 'Tamil Nadu']}}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trend_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'top': {'months': ['February', 'March'],\n",
       "  'age_group': '22',\n",
       "  'regions': [{'state': 'Maharashtra', 'country': ''},\n",
       "   {'state': 'Texas', 'country': ''}]},\n",
       " 'bottom': {'months': ['December', 'January'],\n",
       "  'age_group': '18',\n",
       "  'regions': [{'state': '', 'country': ''},\n",
       "   {'state': 'Berlin', 'country': ''}]}}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_log_refiner_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script Writing Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextScript(BaseModel):\n",
    "    title: str = Field(title=\"Text\", description=\"A very flashy title for the advertisement post.\")\n",
    "    body: str = Field(title=\"Body\", description=\"The body of the advertisement post.\")\n",
    "    month: str = Field(title=\"Month\", description=\"The month for which the advertisement is being created.\")\n",
    "    age_group: str = Field(title=\"Age Group\", description=\"The target age group for the advertisement.\")\n",
    "    region: str = Field(title=\"Region\", description=\"The target region for the advertisement. It should be a full state and Country name. The format should be 'State, Country'.\")\n",
    "    hashtags: List[str] = Field(title=\"Hashtags\", description=\"List of hashtags to be used in the advertisement post. The hashtags should be separated by commas and should contain the '#' symbol.\")\n",
    "\n",
    "class TextScripts(BaseModel):\n",
    "    scripts: List[TextScript] = Field(title=\"Text Scripts\", description=\"List of advertisement text scripts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScriptWriterAgent:\n",
    "    def __init__(self):\n",
    "        self.engine = LangchainJSONEngine(\n",
    "            sampleBaseModel=TextScripts,\n",
    "            systemPromptText=\"\"\"\n",
    "            You are a content writer. Your task is to create an advertisement script for a new product.\n",
    "            \"\"\",\n",
    "            temperature=0.2\n",
    "        )\n",
    "\n",
    "    def run(self, product_description, trend_result, site_log_refiner_result):\n",
    "        prompt0 = f\"\"\" This is the product description: {product_description}.\n",
    "Here are few insights from the trend analysis and site log data. This Trend Data is not Product specific, it is based on the overall market trends of that category.\n",
    "Trend Data: {trend_result}.\n",
    "Here are the insights from log data for the product with particualr product_id with top performing regions, months and age groups.\n",
    "Site Log Data: {site_log_refiner_result['top']}.\n",
    "Now, generate a list of advertisement text scripts for the product based on the insights for each month, age group, and region. \n",
    "Note: The Title and Body of the advertisement should have synergy with other demographics such as age group and region.\n",
    "Note: Here your intent should be bring more and more focus on this demograohy as they are the top performers.\n",
    "        \"\"\"\n",
    "        prompt1 = f\"\"\"This is the product description: {product_description}.\n",
    "Here are few insights from the trend analysis and site log data. This Trend Data is not Product specific, it is based on the overall market trends of that category.\n",
    "Trend Data: {trend_result}.\n",
    "Here are the insights from log data for the product with particualr product_id with least performing regions, months and age groups.\n",
    "Site Log Data: {site_log_refiner_result['bottom']}.\n",
    "Now, generate a list of advertisement text scripts for the product based on the insights for each month, age group, and region.\n",
    "Note: The Title and Body of the advertisement should have synergy with other demographics such as age group and region.\n",
    "Note: Here your intent should be capture the audience from these demograohy as they are the least performers.\n",
    "        \"\"\"\n",
    "        result0 = self.engine.run(prompt0)\n",
    "        result1 = self.engine.run(prompt1)\n",
    "        \n",
    "        result = {\n",
    "            \"top\": [dict(script) for script in result0.scripts],\n",
    "            \"bottom\": [dict(script) for script in result1.scripts]\n",
    "        }\n",
    "\n",
    "        result[\"bucket_id\"] = \"-1\" # Default bucket ID, with no image \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debasmitroy/Desktop/programming/elastic-openserve/backend/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:1413: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "script_writer_agent = ScriptWriterAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_result = script_writer_agent.run(\n",
    "    product_description=description,\n",
    "    trend_result=trend_result,\n",
    "    site_log_refiner_result=site_log_refiner_result\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'top': [{'title': 'Ride the Future with ElecX Pro in Maharashtra, India',\n",
       "   'body': 'Experience the thrill of urban mobility with ElecX Pro, the ultimate electric scooter designed for speed and eco-friendliness. Join the trendsetters in Maharashtra and embrace a sustainable commute. #ElecXPro #UrbanMobility #Maharashtra #India #GreenCommute',\n",
       "   'month': 'February',\n",
       "   'age_group': '22',\n",
       "   'region': 'Maharashtra, India',\n",
       "   'hashtags': ['#ElecXPro',\n",
       "    '#UrbanMobility',\n",
       "    '#Maharashtra',\n",
       "    '#India',\n",
       "    '#GreenCommute']},\n",
       "  {'title': 'Unleash Your Speed in Texas, USA with ElecX Pro',\n",
       "   'body': 'Get ready to conquer the streets of Texas with ElecX Pro, the cutting-edge electric scooter that combines style and performance. Join the urban revolution and ride with power. #ElecXPro #Texas #USA #ElectricRevolution #Speedsters',\n",
       "   'month': 'February',\n",
       "   'age_group': '22',\n",
       "   'region': 'Texas, USA',\n",
       "   'hashtags': ['#ElecXPro',\n",
       "    '#Texas',\n",
       "    '#USA',\n",
       "    '#ElectricRevolution',\n",
       "    '#Speedsters']}],\n",
       " 'bottom': [{'title': 'Unlock the City with ElecX Pro - Exclusive Offer for 18-Year-Olds in Berlin, Germany',\n",
       "   'body': \"Are you ready to ride the future? Introducing ElecX Pro, the ultimate electric scooter for urban adventurers. Zip through the streets of Berlin in style and eco-friendly comfort. Don't miss out on our exclusive offer for 18-year-olds in Berlin, Germany. Limited time only! #ElecXPro #UrbanMobility #BerlinAdventures\",\n",
       "   'month': 'December',\n",
       "   'age_group': '18',\n",
       "   'region': 'Berlin, Germany',\n",
       "   'hashtags': ['#ElecXPro', '#UrbanMobility', '#BerlinAdventures']},\n",
       "  {'title': 'Experience the Thrill of ElecX Pro - Unleash Your Urban Spirit in Berlin, Germany',\n",
       "   'body': \"Get ready to elevate your commute with ElecX Pro, the sleek electric scooter designed for city life. Join the urban movement in Berlin, Germany, and embrace the future of mobility. Don't miss this opportunity to ride in style and sustainability. #ElecXPro #CityLife #BerlinBound\",\n",
       "   'month': 'January',\n",
       "   'age_group': '18',\n",
       "   'region': 'Berlin, Germany',\n",
       "   'hashtags': ['#ElecXPro', '#CityLife', '#BerlinBound']}],\n",
       " 'bucket_id': '-1'}"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generator Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.preview.vision_models import ImageGenerationModel\n",
    "import random\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/debasmitroy/Desktop/programming/gemini-agent-assist/key.json\"\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"hackathon0-project\"\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageGenerator:\n",
    "    def __init__(self):\n",
    "        vertexai.init(project=os.environ[\"GOOGLE_CLOUD_PROJECT\"], location=os.environ[\"GOOGLE_CLOUD_LOCATION\"])\n",
    "        self.model = ImageGenerationModel.from_pretrained(\"imagen-3.0-generate-002\")\n",
    "\n",
    "    def _generate(self,script_result, flag, randomly_genrated_local_bucket_id):\n",
    "        for i,script in enumerate(script_result[flag][:2]):\n",
    "            prompt = f\"\"\"You are image genrator AI Assistant. You are assigned to generate an image for a advertisement company.\n",
    "            Here is the ad details:\n",
    "            Title: {script['title']}\n",
    "            Body: {script['body']}\n",
    "            Month: {script['month']}\n",
    "            Age Group: {script['age_group']}\n",
    "            Region: {script['region']}\n",
    "\n",
    "            Now, generate an image for the advertisement post. The image should be relevant to the product and the target demographics.\n",
    "            \"\"\"\n",
    "            images = self.model.generate_images(\n",
    "                prompt=prompt,\n",
    "                # Optional parameters\n",
    "                number_of_images=1,\n",
    "                language=\"auto\",\n",
    "                # You can't use a seed value and watermark at the same time.\n",
    "                # add_watermark=False,\n",
    "                # seed=100,\n",
    "                aspect_ratio=\"1:1\",\n",
    "                safety_filter_level=\"block_some\",\n",
    "                person_generation=\"allow_adult\",\n",
    "            )\n",
    "\n",
    "            image_name = f\"{script['month']}_{script['age_group']}_{script['region']}_{flag}_{i}.png\"\n",
    "            image_path = f\"assets/{randomly_genrated_local_bucket_id}/{flag}/{image_name}\"\n",
    "\n",
    "            try:\n",
    "                images[0].save(image_path)\n",
    "\n",
    "                print(f\"Image generated for {image_name}. Now delaying for 30 sec to avoid rate limit.\")\n",
    "                time.sleep(30)\n",
    "            except Exception as e:\n",
    "                print(f\"Error while saving image: {e} for {image_name}\")\n",
    "\n",
    "    def run(self,script_result):\n",
    "        randomly_genrated_local_bucket_id = str(random.randint(1000,9999))\n",
    "        os.makedirs(f\"assets/{randomly_genrated_local_bucket_id}/top\", exist_ok=True)\n",
    "        os.makedirs(f\"assets/{randomly_genrated_local_bucket_id}/bottom\", exist_ok=True)\n",
    "\n",
    "        self._generate(script_result, \"top\", randomly_genrated_local_bucket_id)\n",
    "        self._generate(script_result, \"bottom\", randomly_genrated_local_bucket_id)\n",
    "\n",
    "        script_result[\"bucket_id\"] = randomly_genrated_local_bucket_id\n",
    "        return script_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_generator = ImageGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image generated for February_22_Maharashtra, India_top_0.png. Now delaying for 30 sec to avoid rate limit.\n",
      "Image generated for February_22_Texas, USA_top_1.png. Now delaying for 30 sec to avoid rate limit.\n",
      "Error while saving image: list index out of range for December_18_Berlin, Germany_bottom_0.png\n",
      "Image generated for January_18_Berlin, Germany_bottom_1.png. Now delaying for 30 sec to avoid rate limit.\n"
     ]
    }
   ],
   "source": [
    "script_result = image_generator.run(script_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import texttospeech\n",
    "\n",
    "class AudioAgent:\n",
    "    def __init__(self):\n",
    "        self.client = texttospeech.TextToSpeechClient()\n",
    "\n",
    "    def emotional_tts(self, text, speaking_rate=1.0, pitch=0.0, path=\"output.wav\"):\n",
    "        input_text = texttospeech.SynthesisInput(text=text)\n",
    "\n",
    "        voice = texttospeech.VoiceSelectionParams(\n",
    "            language_code=\"en-US\",\n",
    "            name=\"en-US-Wavenet-F\",  # More natural voice\n",
    "            ssml_gender=texttospeech.SsmlVoiceGender.FEMALE,\n",
    "        )\n",
    "\n",
    "        audio_config = texttospeech.AudioConfig(\n",
    "            audio_encoding=texttospeech.AudioEncoding.LINEAR16,  # WAV format\n",
    "            speaking_rate=speaking_rate,\n",
    "            pitch=pitch\n",
    "        )\n",
    "\n",
    "        response = self.client.synthesize_speech(\n",
    "            input=input_text, voice=voice, audio_config=audio_config\n",
    "        )\n",
    "\n",
    "        # Ensure .wav file extension\n",
    "        if not path.endswith('.wav'):\n",
    "            path += '.wav'\n",
    "\n",
    "        with open(path, \"wb\") as out:\n",
    "            out.write(response.audio_content)\n",
    "            print(f'🎧 Audio content written to {path}')\n",
    "\n",
    "    def run(self, script_result):\n",
    "        for i, script in enumerate(script_result['top'][:2]):\n",
    "            text = f\"{script['title']}. {script['body']}.\"\n",
    "            fname = f\"{script['month']}_{script['age_group']}_{script['region']}_top_{i}\"\n",
    "            path = f\"assets/{script_result['bucket_id']}/top/{fname}.wav\"\n",
    "            # Only if the the image exists \n",
    "            if os.path.exists(f\"assets/{script_result['bucket_id']}/top/{fname}.png\"):\n",
    "                self.emotional_tts(text, path=path)\n",
    "\n",
    "        for i, script in enumerate(script_result['bottom'][:2]):\n",
    "            text = f\"{script['title']}. {script['body']}.\"\n",
    "            fname = f\"{script['month']}_{script['age_group']}_{script['region']}_bottom_{i}\"\n",
    "            path = f\"assets/{script_result['bucket_id']}/bottom/{fname}.wav\"\n",
    "            if os.path.exists(f\"assets/{script_result['bucket_id']}/bottom/{fname}.png\"):\n",
    "                self.emotional_tts(text, path=path)\n",
    "\n",
    "        return script_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_agent = AudioAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎧 Audio content written to assets/5180/top/February_22_Maharashtra, India_top_0.wav\n",
      "🎧 Audio content written to assets/5180/top/February_22_Texas, USA_top_1.wav\n",
      "🎧 Audio content written to assets/5180/bottom/January_18_Berlin, Germany_bottom_1.wav\n"
     ]
    }
   ],
   "source": [
    "script_result = audio_agent.run(script_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Email Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.audio import MIMEAudio\n",
    "from email.mime.image import MIMEImage\n",
    "from email.mime.base import MIMEBase\n",
    "from email import encoders\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailAgent:\n",
    "    def __init__(self):\n",
    "        self.SENDER_MAIL = os.environ.get(\"SENDER_MAIL\")\n",
    "        self.SENDER_PASSWORD = os.environ.get(\"SENDER_PASSWORD\")\n",
    "\n",
    "    def send_email_with_attachments(self,sender_email, sender_password, receiver_email, subject, body_text, image_path, audio_path):\n",
    "        # Create the base message\n",
    "        msg = MIMEMultipart()\n",
    "        msg['From'] = sender_email\n",
    "        msg['To'] = receiver_email\n",
    "        msg['Subject'] = subject\n",
    "\n",
    "        # Add text body\n",
    "        msg.attach(MIMEText(body_text, 'plain'))\n",
    "\n",
    "        # Attach image\n",
    "        with open(image_path, 'rb') as img_file:\n",
    "            img = MIMEImage(img_file.read())\n",
    "            img.add_header('Content-Disposition', f'attachment; filename=\"{os.path.basename(image_path)}\"')\n",
    "            msg.attach(img)\n",
    "\n",
    "        # Attach audio\n",
    "        with open(audio_path, 'rb') as audio_file:\n",
    "            audio = MIMEAudio(audio_file.read())\n",
    "            audio.add_header('Content-Disposition', f'attachment; filename=\"{os.path.basename(audio_path)}\"')\n",
    "            msg.attach(audio)\n",
    "\n",
    "        # Connect to Gmail SMTP and send\n",
    "        try:\n",
    "            server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "            server.starttls()\n",
    "            server.login(sender_email, sender_password)\n",
    "            server.send_message(msg)\n",
    "            server.quit()\n",
    "            print(\"✅ Email sent successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to send email: {e}\")\n",
    "\n",
    "    \n",
    "    def run(self, script_result):\n",
    "\n",
    "        for i, script in enumerate(script_result['top'][:2]):\n",
    "            image_path = f\"assets/{script_result['bucket_id']}/top/{script['month']}_{script['age_group']}_{script['region']}_top_{i}.png\"\n",
    "            audio_path = f\"assets/{script_result['bucket_id']}/top/{script['month']}_{script['age_group']}_{script['region']}_top_{i}.wav\"\n",
    "\n",
    "            if os.path.exists(image_path) and os.path.exists(audio_path):\n",
    "                self.send_email_with_attachments(\n",
    "                    sender_email=self.SENDER_MAIL,\n",
    "                    sender_password=self.SENDER_PASSWORD,\n",
    "                    receiver_email=\"kabirrajsingh10@gmail.com\",\n",
    "                    subject=f\"Advertisement for {script['month']} - {script['age_group']} - {script['region']}\",\n",
    "                    body_text=f\"{script['title']}. {script['body']}.\",\n",
    "                    image_path=image_path,\n",
    "                    audio_path=audio_path\n",
    "                )\n",
    "        \n",
    "\n",
    "        for i, script in enumerate(script_result['bottom'][:2]):\n",
    "            image_path = f\"assets/{script_result['bucket_id']}/bottom/{script['month']}_{script['age_group']}_{script['region']}_bottom_{i}.png\"\n",
    "            audio_path = f\"assets/{script_result['bucket_id']}/bottom/{script['month']}_{script['age_group']}_{script['region']}_bottom_{i}.wav\"\n",
    "\n",
    "            if os.path.exists(image_path) and os.path.exists(audio_path):\n",
    "                self.send_email_with_attachments(\n",
    "                    sender_email=self.SENDER_MAIL,\n",
    "                    sender_password=self.SENDER_PASSWORD,\n",
    "                    receiver_email=\"kabirrajsingh10@gmail.com\",\n",
    "                    subject=f\"Advertisement for {script['month']} - {script['age_group']} - {script['region']}\",\n",
    "                    body_text=f\"{script['title']}. {script['body']}.\",\n",
    "                    image_path=image_path,\n",
    "                    audio_path=audio_path\n",
    "                )\n",
    "        script_result['email_sent'] = True\n",
    "        return script_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_agent = EmailAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Email sent successfully!\n",
      "✅ Email sent successfully!\n",
      "✅ Email sent successfully!\n"
     ]
    }
   ],
   "source": [
    "script_result = email_agent.run(script_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"ProductDescription\": description, # Product Description + ProdcutID + Add should be like this\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
